{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperarameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_val_ratio = 0.8\n",
    "\n",
    "epoch = 500\n",
    "batch_size = 8\n",
    "save_best = True\n",
    "\n",
    "start_time = time.time()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PATH"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "outputs": [],
   "source": [
    "MODEL_PATH = './models'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data\n",
    "load data and filled missing value with mean / drop [`id`] [`product_code`]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # Replace str by int using LabelEncoder\n",
    "    data_le = copy.deepcopy(data)\n",
    "    cols = ['attribute_0', 'attribute_1', 'product_code']\n",
    "    for col in cols:\n",
    "        data_le[col] = le.fit_transform(data[col])\n",
    "\n",
    "    data_le = data_le.drop(['id', 'product_code'], axis=1)\n",
    "\n",
    "    # filled missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    final_data = pd.DataFrame(imputer.fit_transform(data_le))\n",
    "\n",
    "    final_data.columns = data_le.columns\n",
    "\n",
    "    return final_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "outputs": [
    {
     "data": {
      "text/plain": "   loading  attribute_0  attribute_1  attribute_2  attribute_3  measurement_0  \\\n0    80.10          1.0          2.0          9.0          5.0            7.0   \n1    84.89          1.0          2.0          9.0          5.0           14.0   \n2    82.43          1.0          2.0          9.0          5.0           12.0   \n3   101.07          1.0          2.0          9.0          5.0           13.0   \n4   188.06          1.0          2.0          9.0          5.0            9.0   \n\n   measurement_1  measurement_2  measurement_3  measurement_4  ...  \\\n0            8.0            4.0         18.040         12.518  ...   \n1            3.0            3.0         18.213         11.540  ...   \n2            1.0            5.0         18.057         11.652  ...   \n3            2.0            6.0         17.295         11.188  ...   \n4            2.0            8.0         19.346         12.950  ...   \n\n   measurement_9  measurement_10  measurement_11  measurement_12  \\\n0         10.672          15.859       17.594000          15.193   \n1         12.448          17.947       17.915000          11.755   \n2         12.715          15.607       19.172085          13.798   \n3         12.471          16.346       18.377000          10.020   \n4         10.337          17.082       19.932000          12.428   \n\n   measurement_13  measurement_14  measurement_15  measurement_16  \\\n0          15.029       16.048444          13.034          14.684   \n1          14.732       15.425000          14.395          15.631   \n2          16.711       18.631000          14.094          17.946   \n3          15.250       15.562000          16.154          17.172   \n4          16.182       12.760000          13.153          16.412   \n\n   measurement_17  failure  \n0         764.100      0.0  \n1         682.057      0.0  \n2         663.376      0.0  \n3         826.282      0.0  \n4         579.885      0.0  \n\n[5 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loading</th>\n      <th>attribute_0</th>\n      <th>attribute_1</th>\n      <th>attribute_2</th>\n      <th>attribute_3</th>\n      <th>measurement_0</th>\n      <th>measurement_1</th>\n      <th>measurement_2</th>\n      <th>measurement_3</th>\n      <th>measurement_4</th>\n      <th>...</th>\n      <th>measurement_9</th>\n      <th>measurement_10</th>\n      <th>measurement_11</th>\n      <th>measurement_12</th>\n      <th>measurement_13</th>\n      <th>measurement_14</th>\n      <th>measurement_15</th>\n      <th>measurement_16</th>\n      <th>measurement_17</th>\n      <th>failure</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>80.10</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>9.0</td>\n      <td>5.0</td>\n      <td>7.0</td>\n      <td>8.0</td>\n      <td>4.0</td>\n      <td>18.040</td>\n      <td>12.518</td>\n      <td>...</td>\n      <td>10.672</td>\n      <td>15.859</td>\n      <td>17.594000</td>\n      <td>15.193</td>\n      <td>15.029</td>\n      <td>16.048444</td>\n      <td>13.034</td>\n      <td>14.684</td>\n      <td>764.100</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>84.89</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>9.0</td>\n      <td>5.0</td>\n      <td>14.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>18.213</td>\n      <td>11.540</td>\n      <td>...</td>\n      <td>12.448</td>\n      <td>17.947</td>\n      <td>17.915000</td>\n      <td>11.755</td>\n      <td>14.732</td>\n      <td>15.425000</td>\n      <td>14.395</td>\n      <td>15.631</td>\n      <td>682.057</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>82.43</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>9.0</td>\n      <td>5.0</td>\n      <td>12.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>18.057</td>\n      <td>11.652</td>\n      <td>...</td>\n      <td>12.715</td>\n      <td>15.607</td>\n      <td>19.172085</td>\n      <td>13.798</td>\n      <td>16.711</td>\n      <td>18.631000</td>\n      <td>14.094</td>\n      <td>17.946</td>\n      <td>663.376</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>101.07</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>9.0</td>\n      <td>5.0</td>\n      <td>13.0</td>\n      <td>2.0</td>\n      <td>6.0</td>\n      <td>17.295</td>\n      <td>11.188</td>\n      <td>...</td>\n      <td>12.471</td>\n      <td>16.346</td>\n      <td>18.377000</td>\n      <td>10.020</td>\n      <td>15.250</td>\n      <td>15.562000</td>\n      <td>16.154</td>\n      <td>17.172</td>\n      <td>826.282</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>188.06</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>9.0</td>\n      <td>5.0</td>\n      <td>9.0</td>\n      <td>2.0</td>\n      <td>8.0</td>\n      <td>19.346</td>\n      <td>12.950</td>\n      <td>...</td>\n      <td>10.337</td>\n      <td>17.082</td>\n      <td>19.932000</td>\n      <td>12.428</td>\n      <td>16.182</td>\n      <td>12.760000</td>\n      <td>13.153</td>\n      <td>16.412</td>\n      <td>579.885</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 24 columns</p>\n</div>"
     },
     "execution_count": 962,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = clean(train_df)\n",
    "test_df_clean = clean(test_df)\n",
    "\n",
    "train_df.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare train data\n",
    "split train data to train and val\n",
    "use dataloader to load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "outputs": [],
   "source": [
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, data, return_y=True):\n",
    "        self.data = data\n",
    "        self.return_y = return_y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.return_y == True:\n",
    "            x = self.data[index][:-1]\n",
    "            y = self.data[index][-1]\n",
    "            return torch.FloatTensor(x), torch.FloatTensor(torch.from_numpy(np.array(y, dtype=np.float32)))\n",
    "        else:\n",
    "            x = self.data[index]\n",
    "            return torch.FloatTensor(x)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "outputs": [],
   "source": [
    "train_data = []\n",
    "val_data = []\n",
    "data_ds = {}\n",
    "dataloaders = {}\n",
    "train_np = train_df.to_numpy()\n",
    "\n",
    "for row in train_np:\n",
    "    if np.random.random() < train_val_ratio:\n",
    "        train_data.append(row)\n",
    "    else:\n",
    "        val_data.append(row)\n",
    "\n",
    "data_ds['train'] = TaskDataset(train_data)\n",
    "data_ds['val'] = TaskDataset(val_data)\n",
    "\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(\n",
    "        data_ds[x],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True) for x in [\n",
    "        'train',\n",
    "        'val']}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 21262, 'val': 5308}\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = {x: len(data_ds[x]) for x in ['train', 'val']}\n",
    "print(dataset_sizes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Model\n",
    "**Model**: input_shape -> 32 -> 64 -> 1\n",
    "**optimizer**: Adam( lr=0.001, betas=( 0.9, 0.999), eps=1e-08 )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "outputs": [],
   "source": [
    "train_accuracy = []\n",
    "train_loss = []\n",
    "val_accuracy = []\n",
    "val_loss = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "outputs": [],
   "source": [
    "def calculate_acc(y_pred, y_test):\n",
    "    y_pred = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred == y_test).sum().float()\n",
    "    acc = correct_results_sum / y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "\n",
    "    return acc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        dataloaders,\n",
    "        optimizer,\n",
    "        num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    min_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        # print('-' * 10)\n",
    "        epoch_since = time.time()\n",
    "        loss_history = []\n",
    "        acc_history = []\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            # Iterate over data.\n",
    "            t = tqdm(enumerate(dataloaders[phase]), total=len(dataloaders[phase]))\n",
    "            for i, (x_train, y_train) in t:\n",
    "                x_train = x_train.to(device)\n",
    "                y_train = y_train.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    y_pred = model(x_train)\n",
    "                    y_train = y_train.unsqueeze(-1)\n",
    "                    loss = criterion(y_pred, y_train)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_train_acc = calculate_acc(y_pred, y_train).item()\n",
    "                acc_history.append(running_train_acc)\n",
    "                loss_history.append(loss.item())\n",
    "                # tqdm settings\n",
    "                epoch_loss = torch.mean(torch.Tensor(loss_history)).item()\n",
    "                epoch_acc = torch.mean(torch.Tensor(acc_history)).item()\n",
    "                #t.set_description(f'epoch_{epoch} {phase} \\t')\n",
    "                t.set_description(f'epoch_{epoch} {phase}  \\t**Acc={epoch_acc/100:.4f}**  Loss={epoch_loss:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "            epoch_loss = torch.mean(torch.Tensor(loss_history)).item()\n",
    "            epoch_acc = torch.mean(torch.Tensor(acc_history)).item()\n",
    "            print(f'{phase} Loss: {epoch_loss:.7f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "\n",
    "            # loop.set_description(f'Epoch [{epoch}/{num_epoch}]')\n",
    "            # loop.set_postfix(loss=loss.item(), acc=running_train_acc)\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                min_loss = epoch_loss\n",
    "                best_epoch = epoch\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            # record loss and accuracy\n",
    "            if phase == 'train':\n",
    "                train_accuracy.append(float(epoch_acc))\n",
    "                train_loss.append(float(epoch_loss))\n",
    "            elif phase == 'val':\n",
    "                val_accuracy.append(float(epoch_acc))\n",
    "                val_loss.append(float(epoch_loss))\n",
    "\n",
    "\n",
    "        epoch_time_elapsed = time.time() - epoch_since\n",
    "        # tqdm.write(\n",
    "        #     f'Time elapsed {epoch_time_elapsed // 60:.0f}m {epoch_time_elapsed % 60:.0f}s\\n')\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    tqdm.write(\n",
    "        f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    tqdm.write(f'Best val Acc: {best_acc:4f} Best epoch: {best_epoch}')\n",
    "\n",
    "    # load best model weights\n",
    "    if save_best:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    torch.save(model, f'{MODEL_PATH}/model.pt')\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_114 train \t**Acc=0.7905**  Loss=0.5069:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1315/2658 [00:07<00:07, 181.85it/s]"
     ]
    }
   ],
   "source": [
    "model = Model(input_shape=train_np.shape[1] - 1)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    betas=(\n",
    "        0.9,\n",
    "        0.999),\n",
    "    eps=1e-08)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_0 train \t**Acc=0.7765**  Loss=0.5844: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 176.19it/s]\n",
      "epoch_0 val \t**Acc=0.7798**  Loss=0.5693: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 255.02it/s]\n",
      "epoch_1 train \t**Acc=0.7861**  Loss=0.5214: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 181.98it/s]\n",
      "epoch_1 val \t**Acc=0.7874**  Loss=0.5182: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 274.15it/s]\n",
      "epoch_2 train \t**Acc=0.7872**  Loss=0.5185: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 185.91it/s]\n",
      "epoch_2 val \t**Acc=0.7883**  Loss=0.5155: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 255.73it/s]\n",
      "epoch_3 train \t**Acc=0.7870**  Loss=0.5164: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.73it/s]\n",
      "epoch_3 val \t**Acc=0.7881**  Loss=0.5139: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 252.75it/s]\n",
      "epoch_4 train \t**Acc=0.7871**  Loss=0.5162: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 178.37it/s]\n",
      "epoch_4 val \t**Acc=0.7882**  Loss=0.5161: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 249.37it/s]\n",
      "epoch_5 train \t**Acc=0.7871**  Loss=0.5145: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 179.25it/s]\n",
      "epoch_5 val \t**Acc=0.7882**  Loss=0.5120: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 249.16it/s]\n",
      "epoch_6 train \t**Acc=0.7870**  Loss=0.5140: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 184.50it/s]\n",
      "epoch_6 val \t**Acc=0.7881**  Loss=0.5120: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 260.60it/s]\n",
      "epoch_7 train \t**Acc=0.7871**  Loss=0.5138: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 185.14it/s]\n",
      "epoch_7 val \t**Acc=0.7883**  Loss=0.5117: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 256.46it/s]\n",
      "epoch_8 train \t**Acc=0.7871**  Loss=0.5137: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 185.86it/s]\n",
      "epoch_8 val \t**Acc=0.7882**  Loss=0.5124: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 256.50it/s]\n",
      "epoch_9 train \t**Acc=0.7870**  Loss=0.5129: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 186.63it/s]\n",
      "epoch_9 val \t**Acc=0.7882**  Loss=0.5111: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 252.67it/s]\n",
      "epoch_10 train \t**Acc=0.7870**  Loss=0.5126: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 186.53it/s]\n",
      "epoch_10 val \t**Acc=0.7882**  Loss=0.5109: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 251.24it/s]\n",
      "epoch_11 train \t**Acc=0.7870**  Loss=0.5129: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 186.27it/s]\n",
      "epoch_11 val \t**Acc=0.7882**  Loss=0.5112: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 254.39it/s]\n",
      "epoch_12 train \t**Acc=0.7870**  Loss=0.5126: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 185.84it/s]\n",
      "epoch_12 val \t**Acc=0.7883**  Loss=0.5104: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 245.60it/s]\n",
      "epoch_13 train \t**Acc=0.7871**  Loss=0.5128: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 184.26it/s]\n",
      "epoch_13 val \t**Acc=0.7881**  Loss=0.5108: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 253.49it/s]\n",
      "epoch_14 train \t**Acc=0.7870**  Loss=0.5125: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 184.30it/s]\n",
      "epoch_14 val \t**Acc=0.7882**  Loss=0.5107: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 249.55it/s]\n",
      "epoch_15 train \t**Acc=0.7871**  Loss=0.5124: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 185.25it/s]\n",
      "epoch_15 val \t**Acc=0.7883**  Loss=0.5127: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 257.00it/s]\n",
      "epoch_16 train \t**Acc=0.7869**  Loss=0.5124: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 185.28it/s]\n",
      "epoch_16 val \t**Acc=0.7881**  Loss=0.5113: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 253.94it/s]\n",
      "epoch_17 train \t**Acc=0.7870**  Loss=0.5122: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 184.79it/s]\n",
      "epoch_17 val \t**Acc=0.7882**  Loss=0.5103: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 253.14it/s]\n",
      "epoch_18 train \t**Acc=0.7871**  Loss=0.5122: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 184.67it/s]\n",
      "epoch_18 val \t**Acc=0.7883**  Loss=0.5111: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 254.16it/s]\n",
      "epoch_19 train \t**Acc=0.7871**  Loss=0.5124: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 185.91it/s]\n",
      "epoch_19 val \t**Acc=0.7882**  Loss=0.5110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 252.54it/s]\n",
      "epoch_20 train \t**Acc=0.7870**  Loss=0.5121: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 184.02it/s]\n",
      "epoch_20 val \t**Acc=0.7881**  Loss=0.5103: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 249.80it/s]\n",
      "epoch_21 train \t**Acc=0.7870**  Loss=0.5117: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 185.37it/s]\n",
      "epoch_21 val \t**Acc=0.7881**  Loss=0.5098: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 250.51it/s]\n",
      "epoch_22 train \t**Acc=0.7871**  Loss=0.5121: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.90it/s]\n",
      "epoch_22 val \t**Acc=0.7883**  Loss=0.5104: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 256.57it/s]\n",
      "epoch_23 train \t**Acc=0.7871**  Loss=0.5123: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.88it/s]\n",
      "epoch_23 val \t**Acc=0.7882**  Loss=0.5106: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 250.73it/s]\n",
      "epoch_24 train \t**Acc=0.7871**  Loss=0.5121: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 181.90it/s]\n",
      "epoch_24 val \t**Acc=0.7882**  Loss=0.5102: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 242.46it/s]\n",
      "epoch_25 train \t**Acc=0.7871**  Loss=0.5119: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.81it/s]\n",
      "epoch_25 val \t**Acc=0.7883**  Loss=0.5102: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 250.35it/s]\n",
      "epoch_26 train \t**Acc=0.7870**  Loss=0.5118: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.99it/s]\n",
      "epoch_26 val \t**Acc=0.7882**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 254.11it/s]\n",
      "epoch_27 train \t**Acc=0.7870**  Loss=0.5125: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.94it/s]\n",
      "epoch_27 val \t**Acc=0.7882**  Loss=0.5106: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 247.98it/s]\n",
      "epoch_28 train \t**Acc=0.7871**  Loss=0.5118: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.04it/s]\n",
      "epoch_28 val \t**Acc=0.7882**  Loss=0.5101: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 249.71it/s]\n",
      "epoch_29 train \t**Acc=0.7870**  Loss=0.5117: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.79it/s]\n",
      "epoch_29 val \t**Acc=0.7881**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 248.06it/s]\n",
      "epoch_30 train \t**Acc=0.7870**  Loss=0.5118: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 184.45it/s]\n",
      "epoch_30 val \t**Acc=0.7881**  Loss=0.5102: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 245.51it/s]\n",
      "epoch_31 train \t**Acc=0.7871**  Loss=0.5117: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.26it/s]\n",
      "epoch_31 val \t**Acc=0.7882**  Loss=0.5098: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 245.87it/s]\n",
      "epoch_32 train \t**Acc=0.7871**  Loss=0.5120: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 184.35it/s]\n",
      "epoch_32 val \t**Acc=0.7883**  Loss=0.5101: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 253.59it/s]\n",
      "epoch_33 train \t**Acc=0.7871**  Loss=0.5116: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.50it/s]\n",
      "epoch_33 val \t**Acc=0.7883**  Loss=0.5106: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 246.95it/s]\n",
      "epoch_34 train \t**Acc=0.7872**  Loss=0.5119: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 184.42it/s]\n",
      "epoch_34 val \t**Acc=0.7883**  Loss=0.5105: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 245.08it/s]\n",
      "epoch_35 train \t**Acc=0.7870**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 184.44it/s]\n",
      "epoch_35 val \t**Acc=0.7881**  Loss=0.5105: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 244.65it/s]\n",
      "epoch_36 train \t**Acc=0.7870**  Loss=0.5123: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 184.10it/s]\n",
      "epoch_36 val \t**Acc=0.7881**  Loss=0.5103: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 254.95it/s]\n",
      "epoch_37 train \t**Acc=0.7869**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 182.73it/s]\n",
      "epoch_37 val \t**Acc=0.7882**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 246.92it/s]\n",
      "epoch_38 train \t**Acc=0.7870**  Loss=0.5117: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 178.90it/s]\n",
      "epoch_38 val \t**Acc=0.7882**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 236.32it/s]\n",
      "epoch_39 train \t**Acc=0.7870**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 182.84it/s]\n",
      "epoch_39 val \t**Acc=0.7881**  Loss=0.5097: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 241.21it/s]\n",
      "epoch_40 train \t**Acc=0.7871**  Loss=0.5116: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 184.33it/s]\n",
      "epoch_40 val \t**Acc=0.7882**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 246.88it/s]\n",
      "epoch_41 train \t**Acc=0.7870**  Loss=0.5122: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.30it/s]\n",
      "epoch_41 val \t**Acc=0.7882**  Loss=0.5102: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 239.26it/s]\n",
      "epoch_42 train \t**Acc=0.7871**  Loss=0.5117: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 184.33it/s]\n",
      "epoch_42 val \t**Acc=0.7882**  Loss=0.5098: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 242.04it/s]\n",
      "epoch_43 train \t**Acc=0.7871**  Loss=0.5117: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 185.01it/s]\n",
      "epoch_43 val \t**Acc=0.7882**  Loss=0.5098: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 248.96it/s]\n",
      "epoch_44 train \t**Acc=0.7872**  Loss=0.5118: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 184.85it/s]\n",
      "epoch_44 val \t**Acc=0.7884**  Loss=0.5100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 247.10it/s]\n",
      "epoch_45 train \t**Acc=0.7870**  Loss=0.5116: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.75it/s]\n",
      "epoch_45 val \t**Acc=0.7882**  Loss=0.5106: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 245.13it/s]\n",
      "epoch_46 train \t**Acc=0.7870**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 180.31it/s]\n",
      "epoch_46 val \t**Acc=0.7882**  Loss=0.5095: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 254.12it/s]\n",
      "epoch_47 train \t**Acc=0.7870**  Loss=0.5119: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 180.89it/s]\n",
      "epoch_47 val \t**Acc=0.7882**  Loss=0.5104: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 248.64it/s]\n",
      "epoch_48 train \t**Acc=0.7870**  Loss=0.5112: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 176.17it/s]\n",
      "epoch_48 val \t**Acc=0.7883**  Loss=0.5096: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 237.75it/s]\n",
      "epoch_49 train \t**Acc=0.7871**  Loss=0.5112: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 182.51it/s]\n",
      "epoch_49 val \t**Acc=0.7883**  Loss=0.5096: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 246.65it/s]\n",
      "epoch_50 train \t**Acc=0.7872**  Loss=0.5112: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.65it/s]\n",
      "epoch_50 val \t**Acc=0.7883**  Loss=0.5093: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 241.79it/s]\n",
      "epoch_51 train \t**Acc=0.7871**  Loss=0.5120: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.57it/s]\n",
      "epoch_51 val \t**Acc=0.7882**  Loss=0.5101: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 245.90it/s]\n",
      "epoch_52 train \t**Acc=0.7871**  Loss=0.5120: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.48it/s]\n",
      "epoch_52 val \t**Acc=0.7882**  Loss=0.5102: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 249.52it/s]\n",
      "epoch_53 train \t**Acc=0.7870**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.22it/s]\n",
      "epoch_53 val \t**Acc=0.7882**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 239.01it/s]\n",
      "epoch_54 train \t**Acc=0.7870**  Loss=0.5122: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 182.63it/s]\n",
      "epoch_54 val \t**Acc=0.7882**  Loss=0.5103: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 248.83it/s]\n",
      "epoch_55 train \t**Acc=0.7870**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 181.31it/s]\n",
      "epoch_55 val \t**Acc=0.7882**  Loss=0.5097: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 242.86it/s]\n",
      "epoch_56 train \t**Acc=0.7870**  Loss=0.5123: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.23it/s]\n",
      "epoch_56 val \t**Acc=0.7881**  Loss=0.5104: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 250.52it/s]\n",
      "epoch_57 train \t**Acc=0.7870**  Loss=0.5119: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 182.00it/s]\n",
      "epoch_57 val \t**Acc=0.7881**  Loss=0.5102: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 255.79it/s]\n",
      "epoch_58 train \t**Acc=0.7872**  Loss=0.5118: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 182.72it/s]\n",
      "epoch_58 val \t**Acc=0.7883**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 247.32it/s]\n",
      "epoch_59 train \t**Acc=0.7870**  Loss=0.5116: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 181.84it/s]\n",
      "epoch_59 val \t**Acc=0.7882**  Loss=0.5098: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 242.54it/s]\n",
      "epoch_60 train \t**Acc=0.7872**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 182.20it/s]\n",
      "epoch_60 val \t**Acc=0.7884**  Loss=0.5095: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 245.84it/s]\n",
      "epoch_61 train \t**Acc=0.7870**  Loss=0.5117: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 181.81it/s]\n",
      "epoch_61 val \t**Acc=0.7882**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 242.67it/s]\n",
      "epoch_62 train \t**Acc=0.7870**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 182.66it/s]\n",
      "epoch_62 val \t**Acc=0.7882**  Loss=0.5101: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 255.44it/s]\n",
      "epoch_63 train \t**Acc=0.7870**  Loss=0.5117: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 181.64it/s]\n",
      "epoch_63 val \t**Acc=0.7882**  Loss=0.5103: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 245.12it/s]\n",
      "epoch_64 train \t**Acc=0.7870**  Loss=0.5118: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 182.71it/s]\n",
      "epoch_64 val \t**Acc=0.7882**  Loss=0.5100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 241.94it/s]\n",
      "epoch_65 train \t**Acc=0.7871**  Loss=0.5116: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.66it/s]\n",
      "epoch_65 val \t**Acc=0.7882**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 252.37it/s]\n",
      "epoch_66 train \t**Acc=0.7871**  Loss=0.5116: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 181.95it/s]\n",
      "epoch_66 val \t**Acc=0.7883**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 247.15it/s]\n",
      "epoch_67 train \t**Acc=0.7871**  Loss=0.5119: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 181.85it/s]\n",
      "epoch_67 val \t**Acc=0.7883**  Loss=0.5100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 238.19it/s]\n",
      "epoch_68 train \t**Acc=0.7870**  Loss=0.5120: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 181.12it/s]\n",
      "epoch_68 val \t**Acc=0.7882**  Loss=0.5101: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 254.18it/s]\n",
      "epoch_69 train \t**Acc=0.7871**  Loss=0.5118: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 181.36it/s]\n",
      "epoch_69 val \t**Acc=0.7882**  Loss=0.5100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 247.88it/s]\n",
      "epoch_70 train \t**Acc=0.7872**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.28it/s]\n",
      "epoch_70 val \t**Acc=0.7884**  Loss=0.5097: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 241.06it/s]\n",
      "epoch_71 train \t**Acc=0.7870**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 182.82it/s]\n",
      "epoch_71 val \t**Acc=0.7882**  Loss=0.5107: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 246.12it/s]\n",
      "epoch_72 train \t**Acc=0.7870**  Loss=0.5117: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 180.36it/s]\n",
      "epoch_72 val \t**Acc=0.7882**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 240.65it/s]\n",
      "epoch_73 train \t**Acc=0.7871**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 172.40it/s]\n",
      "epoch_73 val \t**Acc=0.7881**  Loss=0.5095: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 245.00it/s]\n",
      "epoch_74 train \t**Acc=0.7870**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 170.06it/s]\n",
      "epoch_74 val \t**Acc=0.7882**  Loss=0.5097: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 249.33it/s]\n",
      "epoch_75 train \t**Acc=0.7870**  Loss=0.5119: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:16<00:00, 162.07it/s]\n",
      "epoch_75 val \t**Acc=0.7882**  Loss=0.5106: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 243.87it/s]\n",
      "epoch_76 train \t**Acc=0.7870**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 171.77it/s]\n",
      "epoch_76 val \t**Acc=0.7881**  Loss=0.5096: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:03<00:00, 219.96it/s]\n",
      "epoch_77 train \t**Acc=0.7871**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 170.85it/s]\n",
      "epoch_77 val \t**Acc=0.7882**  Loss=0.5096: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 243.09it/s]\n",
      "epoch_78 train \t**Acc=0.7871**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 172.06it/s]\n",
      "epoch_78 val \t**Acc=0.7883**  Loss=0.5096: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 236.03it/s]\n",
      "epoch_79 train \t**Acc=0.7870**  Loss=0.5118: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 170.18it/s]\n",
      "epoch_79 val \t**Acc=0.7883**  Loss=0.5100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 238.27it/s]\n",
      "epoch_80 train \t**Acc=0.7872**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 172.35it/s]\n",
      "epoch_80 val \t**Acc=0.7883**  Loss=0.5098: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 256.36it/s]\n",
      "epoch_81 train \t**Acc=0.7871**  Loss=0.5113: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 177.15it/s]\n",
      "epoch_81 val \t**Acc=0.7883**  Loss=0.5104: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 233.90it/s]\n",
      "epoch_82 train \t**Acc=0.7870**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 170.70it/s]\n",
      "epoch_82 val \t**Acc=0.7881**  Loss=0.5102: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 238.80it/s]\n",
      "epoch_83 train \t**Acc=0.7870**  Loss=0.5118: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 169.21it/s]\n",
      "epoch_83 val \t**Acc=0.7882**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 230.19it/s]\n",
      "epoch_84 train \t**Acc=0.7871**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 174.70it/s]\n",
      "epoch_84 val \t**Acc=0.7883**  Loss=0.5102: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 244.71it/s]\n",
      "epoch_85 train \t**Acc=0.7870**  Loss=0.5116: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 170.18it/s]\n",
      "epoch_85 val \t**Acc=0.7882**  Loss=0.5100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 241.28it/s]\n",
      "epoch_86 train \t**Acc=0.7871**  Loss=0.5113: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 173.57it/s]\n",
      "epoch_86 val \t**Acc=0.7883**  Loss=0.5096: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 226.15it/s]\n",
      "epoch_87 train \t**Acc=0.7871**  Loss=0.5112: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 173.24it/s]\n",
      "epoch_87 val \t**Acc=0.7883**  Loss=0.5096: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 236.39it/s]\n",
      "epoch_88 train \t**Acc=0.7871**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 173.75it/s]\n",
      "epoch_88 val \t**Acc=0.7883**  Loss=0.5095: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 250.48it/s]\n",
      "epoch_89 train \t**Acc=0.7870**  Loss=0.5112: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 180.50it/s]\n",
      "epoch_89 val \t**Acc=0.7882**  Loss=0.5095: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 273.96it/s]\n",
      "epoch_90 train \t**Acc=0.7870**  Loss=0.5112: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 180.27it/s]\n",
      "epoch_90 val \t**Acc=0.7882**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 272.45it/s]\n",
      "epoch_91 train \t**Acc=0.7871**  Loss=0.5119: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:16<00:00, 163.16it/s]\n",
      "epoch_91 val \t**Acc=0.7882**  Loss=0.5101: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 237.33it/s]\n",
      "epoch_92 train \t**Acc=0.7871**  Loss=0.5117: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 172.80it/s]\n",
      "epoch_92 val \t**Acc=0.7883**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 248.19it/s]\n",
      "epoch_93 train \t**Acc=0.7871**  Loss=0.5117: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 177.37it/s]\n",
      "epoch_93 val \t**Acc=0.7884**  Loss=0.5100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 248.26it/s]\n",
      "epoch_94 train \t**Acc=0.7871**  Loss=0.5118: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 170.96it/s]\n",
      "epoch_94 val \t**Acc=0.7882**  Loss=0.5102: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 247.03it/s]\n",
      "epoch_95 train \t**Acc=0.7871**  Loss=0.5113: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 172.94it/s]\n",
      "epoch_95 val \t**Acc=0.7883**  Loss=0.5095: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 309.09it/s]\n",
      "epoch_96 train \t**Acc=0.7870**  Loss=0.5119: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 176.97it/s]\n",
      "epoch_96 val \t**Acc=0.7881**  Loss=0.5107: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 294.46it/s]\n",
      "epoch_97 train \t**Acc=0.7870**  Loss=0.5108: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 180.98it/s]\n",
      "epoch_97 val \t**Acc=0.7882**  Loss=0.5092: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 272.63it/s]\n",
      "epoch_98 train \t**Acc=0.7871**  Loss=0.5119: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:13<00:00, 190.96it/s]\n",
      "epoch_98 val \t**Acc=0.7882**  Loss=0.5104: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 242.42it/s]\n",
      "epoch_99 train \t**Acc=0.7872**  Loss=0.5129: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:16<00:00, 164.91it/s]\n",
      "epoch_99 val \t**Acc=0.7883**  Loss=0.5120: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 282.59it/s]\n",
      "epoch_100 train \t**Acc=0.7871**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 173.32it/s]\n",
      "epoch_100 val \t**Acc=0.7883**  Loss=0.5096: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 264.18it/s]\n",
      "epoch_101 train \t**Acc=0.7870**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 181.22it/s]\n",
      "epoch_101 val \t**Acc=0.7882**  Loss=0.5098: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 312.87it/s]\n",
      "epoch_102 train \t**Acc=0.7871**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 188.90it/s]\n",
      "epoch_102 val \t**Acc=0.7883**  Loss=0.5095: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 303.38it/s]\n",
      "epoch_103 train \t**Acc=0.7871**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:13<00:00, 193.50it/s]\n",
      "epoch_103 val \t**Acc=0.7882**  Loss=0.5098: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 284.52it/s]\n",
      "epoch_104 train \t**Acc=0.7870**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 176.79it/s]\n",
      "epoch_104 val \t**Acc=0.7882**  Loss=0.5095: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 302.37it/s]\n",
      "epoch_105 train \t**Acc=0.7871**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 189.38it/s]\n",
      "epoch_105 val \t**Acc=0.7883**  Loss=0.5095: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 288.46it/s]\n",
      "epoch_106 train \t**Acc=0.7871**  Loss=0.5111: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 188.33it/s]\n",
      "epoch_106 val \t**Acc=0.7882**  Loss=0.5095: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 260.41it/s]\n",
      "epoch_107 train \t**Acc=0.7872**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 188.83it/s]\n",
      "epoch_107 val \t**Acc=0.7883**  Loss=0.5095: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 271.81it/s]\n",
      "epoch_108 train \t**Acc=0.7870**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 178.58it/s]\n",
      "epoch_108 val \t**Acc=0.7883**  Loss=0.5100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 228.78it/s]\n",
      "epoch_109 train \t**Acc=0.7871**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 168.76it/s]\n",
      "epoch_109 val \t**Acc=0.7883**  Loss=0.5097: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 302.12it/s]\n",
      "epoch_110 train \t**Acc=0.7870**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 176.87it/s]\n",
      "epoch_110 val \t**Acc=0.7882**  Loss=0.5098: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 265.96it/s]\n",
      "epoch_111 train \t**Acc=0.7871**  Loss=0.5112: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.14it/s]\n",
      "epoch_111 val \t**Acc=0.7883**  Loss=0.5096: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 268.13it/s]\n",
      "epoch_112 train \t**Acc=0.7870**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 183.63it/s]\n",
      "epoch_112 val \t**Acc=0.7882**  Loss=0.5104: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 291.83it/s]\n",
      "epoch_113 train \t**Acc=0.7870**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 174.26it/s]\n",
      "epoch_113 val \t**Acc=0.7882**  Loss=0.5097: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 254.42it/s]\n",
      "epoch_114 train \t**Acc=0.7870**  Loss=0.5112: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 177.07it/s]\n",
      "epoch_114 val \t**Acc=0.7882**  Loss=0.5096: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 247.84it/s]\n",
      "epoch_115 train \t**Acc=0.7870**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 181.79it/s]\n",
      "epoch_115 val \t**Acc=0.7882**  Loss=0.5097: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 253.19it/s]\n",
      "epoch_116 train \t**Acc=0.7872**  Loss=0.5112: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 182.07it/s]\n",
      "epoch_116 val \t**Acc=0.7883**  Loss=0.5093: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 247.84it/s]\n",
      "epoch_117 train \t**Acc=0.7870**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 178.03it/s]\n",
      "epoch_117 val \t**Acc=0.7882**  Loss=0.5095: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 253.02it/s]\n",
      "epoch_118 train \t**Acc=0.7870**  Loss=0.5111: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 179.45it/s]\n",
      "epoch_118 val \t**Acc=0.7882**  Loss=0.5093: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 246.41it/s]\n",
      "epoch_119 train \t**Acc=0.7871**  Loss=0.5116: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 176.09it/s]\n",
      "epoch_119 val \t**Acc=0.7882**  Loss=0.5098: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 264.04it/s]\n",
      "epoch_120 train \t**Acc=0.7871**  Loss=0.5113: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 177.08it/s]\n",
      "epoch_120 val \t**Acc=0.7883**  Loss=0.5095: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 253.31it/s]\n",
      "epoch_121 train \t**Acc=0.7871**  Loss=0.5116: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 176.00it/s]\n",
      "epoch_121 val \t**Acc=0.7883**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 254.68it/s]\n",
      "epoch_122 train \t**Acc=0.7872**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 176.28it/s]\n",
      "epoch_122 val \t**Acc=0.7883**  Loss=0.5101: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 243.05it/s]\n",
      "epoch_123 train \t**Acc=0.7871**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 176.06it/s]\n",
      "epoch_123 val \t**Acc=0.7882**  Loss=0.5097: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 251.35it/s]\n",
      "epoch_124 train \t**Acc=0.7870**  Loss=0.5119: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 176.83it/s]\n",
      "epoch_124 val \t**Acc=0.7882**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 240.85it/s]\n",
      "epoch_125 train \t**Acc=0.7871**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 177.68it/s]\n",
      "epoch_125 val \t**Acc=0.7882**  Loss=0.5098: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 255.07it/s]\n",
      "epoch_126 train \t**Acc=0.7870**  Loss=0.5121: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 179.69it/s]\n",
      "epoch_126 val \t**Acc=0.7882**  Loss=0.5103: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 244.99it/s]\n",
      "epoch_127 train \t**Acc=0.7871**  Loss=0.5119: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 184.08it/s]\n",
      "epoch_127 val \t**Acc=0.7883**  Loss=0.5107: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 243.85it/s]\n",
      "epoch_128 train \t**Acc=0.7871**  Loss=0.5112: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:16<00:00, 164.70it/s]\n",
      "epoch_128 val \t**Acc=0.7882**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:03<00:00, 221.05it/s]\n",
      "epoch_129 train \t**Acc=0.7871**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:16<00:00, 161.83it/s]\n",
      "epoch_129 val \t**Acc=0.7883**  Loss=0.5094: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 233.29it/s]\n",
      "epoch_130 train \t**Acc=0.7871**  Loss=0.5119: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 171.92it/s]\n",
      "epoch_130 val \t**Acc=0.7882**  Loss=0.5100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 236.83it/s]\n",
      "epoch_131 train \t**Acc=0.7871**  Loss=0.5112: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 177.90it/s]\n",
      "epoch_131 val \t**Acc=0.7883**  Loss=0.5095: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 255.40it/s]\n",
      "epoch_132 train \t**Acc=0.7871**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 188.73it/s]\n",
      "epoch_132 val \t**Acc=0.7882**  Loss=0.5100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 261.20it/s]\n",
      "epoch_133 train \t**Acc=0.7871**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 177.89it/s]\n",
      "epoch_133 val \t**Acc=0.7883**  Loss=0.5098: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 252.43it/s]\n",
      "epoch_134 train \t**Acc=0.7871**  Loss=0.5112: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 174.54it/s]\n",
      "epoch_134 val \t**Acc=0.7882**  Loss=0.5095: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 245.31it/s]\n",
      "epoch_135 train \t**Acc=0.7871**  Loss=0.5112: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 181.06it/s]\n",
      "epoch_135 val \t**Acc=0.7882**  Loss=0.5095: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 277.89it/s]\n",
      "epoch_136 train \t**Acc=0.7871**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 179.47it/s]\n",
      "epoch_136 val \t**Acc=0.7882**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 288.82it/s]\n",
      "epoch_137 train \t**Acc=0.7871**  Loss=0.5114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 188.43it/s]\n",
      "epoch_137 val \t**Acc=0.7883**  Loss=0.5098: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 251.24it/s]\n",
      "epoch_138 train \t**Acc=0.7871**  Loss=0.5121: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 181.84it/s]\n",
      "epoch_138 val \t**Acc=0.7883**  Loss=0.5104: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 268.88it/s]\n",
      "epoch_139 train \t**Acc=0.7870**  Loss=0.5117: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 187.66it/s]\n",
      "epoch_139 val \t**Acc=0.7882**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 262.08it/s]\n",
      "epoch_140 train \t**Acc=0.7871**  Loss=0.5117: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 172.59it/s]\n",
      "epoch_140 val \t**Acc=0.7883**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 240.64it/s]\n",
      "epoch_141 train \t**Acc=0.7871**  Loss=0.5118: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 174.87it/s]\n",
      "epoch_141 val \t**Acc=0.7882**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 298.61it/s]\n",
      "epoch_142 train \t**Acc=0.7870**  Loss=0.5113: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 184.27it/s]\n",
      "epoch_142 val \t**Acc=0.7881**  Loss=0.5098: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 301.29it/s]\n",
      "epoch_143 val \t**Acc=0.7882**  Loss=0.5098: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 304.68it/s]t/s]\n",
      "epoch_144 train \t**Acc=0.7870**  Loss=0.5111: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 186.70it/s]\n",
      "epoch_144 val \t**Acc=0.7882**  Loss=0.5093: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 320.45it/s]\n",
      "epoch_143 train \t**Acc=0.7870**  Loss=0.5117: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 177.99it/s]\n",
      "epoch_145 train \t**Acc=0.7869**  Loss=0.5116: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 173.66it/s]\n",
      "epoch_145 val \t**Acc=0.7881**  Loss=0.5097: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 250.35it/s]\n",
      "epoch_146 train \t**Acc=0.7870**  Loss=0.5116: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 171.34it/s]\n",
      "epoch_146 val \t**Acc=0.7882**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 241.73it/s]\n",
      "epoch_147 train \t**Acc=0.7871**  Loss=0.5113: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 173.46it/s]\n",
      "epoch_147 val \t**Acc=0.7883**  Loss=0.5098: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 250.54it/s]\n",
      "epoch_148 train \t**Acc=0.7872**  Loss=0.5111: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:16<00:00, 164.89it/s]\n",
      "epoch_148 val \t**Acc=0.7883**  Loss=0.5095: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 229.05it/s]\n",
      "epoch_149 train \t**Acc=0.7871**  Loss=0.5122: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 187.94it/s]\n",
      "epoch_149 val \t**Acc=0.7883**  Loss=0.5105: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 257.59it/s]\n",
      "epoch_150 train \t**Acc=0.7872**  Loss=0.5117: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 172.90it/s]\n",
      "epoch_150 val \t**Acc=0.7883**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 250.48it/s]\n",
      "epoch_151 train \t**Acc=0.7870**  Loss=0.5117: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 170.60it/s]\n",
      "epoch_151 val \t**Acc=0.7882**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 249.61it/s]\n",
      "epoch_152 train \t**Acc=0.7869**  Loss=0.5115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:15<00:00, 173.04it/s]\n",
      "epoch_152 val \t**Acc=0.7881**  Loss=0.5095: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 246.53it/s]\n",
      "epoch_153 train \t**Acc=0.7870**  Loss=0.5119: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 181.37it/s]\n",
      "epoch_153 val \t**Acc=0.7883**  Loss=0.5103: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 274.12it/s]\n",
      "epoch_154 train \t**Acc=0.7871**  Loss=0.5117: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 182.28it/s]\n",
      "epoch_154 val \t**Acc=0.7883**  Loss=0.5100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 269.70it/s]\n",
      "epoch_155 train \t**Acc=0.7869**  Loss=0.5120: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [00:14<00:00, 180.18it/s]\n",
      "epoch_155 val \t**Acc=0.7881**  Loss=0.5099: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 262.18it/s]\n",
      "epoch_156 train \t**Acc=0.7842**  Loss=0.5175:  23%|â–ˆâ–ˆâ–Ž       | 602/2658 [00:03<00:10, 189.11it/s]"
     ]
    }
   ],
   "source": [
    "model_ft1 = train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    dataloaders,\n",
    "    optimizer,\n",
    "    num_epochs=epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#printing the loss\n",
    "plt.plot(val_loss)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('loss')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#printing the accuracy\n",
    "plt.plot(val_accuracy)\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data = test_df_clean.to_numpy()\n",
    "test_ds = TaskDataset(test_data, return_y=False)\n",
    "print(\"test num: \", test_ds.__len__())\n",
    "test_dl = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    "    shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "pred = []\n",
    "for x in tqdm(test_dl):\n",
    "    x = x.to(device)\n",
    "    y_pred = model(x)\n",
    "    output = torch.sigmoid(y_pred)\n",
    "    output = output.cpu().detach().numpy()\n",
    "    for i in range(len(output)):\n",
    "        pred.append(output[i][0])\n",
    "result = pd.DataFrame({'id': test_df['id'], 'failure': pred})\n",
    "result.to_csv('submission.csv', index=0)\n",
    "result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "process_time = time.time() - start_time\n",
    "print(\n",
    "    f'\\n###############################\\n'\n",
    "    f'Process complete in {process_time // 60:.0f}m {process_time % 60:.0f}s')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
